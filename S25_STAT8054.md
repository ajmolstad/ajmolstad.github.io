---
layout: defaultCourse
title: STAT8054
permalink: /S25_STAT8054/
---

# STAT 8054: Advanced Statistical Computing
**Instructor.** Aaron J. Molstad (*amolstad@umn.edu*)  
**Office hours.** Monday 3:00 - 4:00PM and Wednesday 9:00 - 10:00AM in Ford Hall 384

**Syllabus.** [[pdf](https://canvas.umn.edu/files/49831537/download?download_frd=1)]   
**Lecture.** Monday, Wednesday, Friday at 12:20 - 1:10PM in Ford Hall 170 

Note that you must be logged into your UMN Canvas account to access course materials.   

----------------------

|| **Lecture** ||  **Topics** ||  
|| 1.1 (1/22)  || Course overview [[slides](https://canvas.umn.edu/files/49831522/download?download_frd=1)] ||   
|| 1.2 (1/24)  || Fundamentals of numerical linear algebra [[slides](https://canvas.umn.edu/files/49893294/download?download_frd=1)]||    
|| 1.3 (1/27)  || Computational complexity, matrix decompositions [[slides](https://canvas.umn.edu/files/49962875/download?download_frd=1)] ||  

Lecture 1 [[notes](https://canvas.umn.edu/files/50044826/download?download_frd=1)]

----------------------

|| 2.1 (1/29)  || Unconstrained optimization overview [[slides](https://canvas.umn.edu/files/50039527/download?download_frd=1)] ||   
|| 2.2 (1/31)  || Optimality conditions, convexity [[slides](https://canvas.umn.edu/files/50104516/download?download_frd=1)] ||   
|| 2.3 (2/3)  || Quasiconvexity, strong convexity, L-smoothness [[slides](https://canvas.umn.edu/files/50179163/download?download_frd=1) || 

Lecture 2 [[notes](https://canvas.umn.edu/files/50330601/download?download_frd=1)]

-----------------------

|| 3.1 (2/5)  || Steepest descent, gradient descent [[slides](https://canvas.umn.edu/files/50261601/download?download_frd=1)] ||  
|| 3.2 (2/7) || Example, accelerated gradient descent [[slides](https://canvas.umn.edu/files/50330595/download?download_frd=1)] ||  
|| 3.3 (2/10) || Newton's method [[slides](https://canvas.umn.edu/files/50414285/download?download_frd=1)] || 

Lecture 3 [[notes](https://canvas.umn.edu/files/50528849/download?download_frd=1)]


-----------------------


|| 4.1 (2/12) || Majorize-minimize principle [[slides](https://canvas.umn.edu/files/50495130/download?download_frd=1)] ||  
|| 4.2 (2/14) || Expectation-maximization algorithm [[slides](https://canvas.umn.edu/files/50564479/download?download_frd=1)] ||  
|| 4.3 (2/17) || Proximal gradient descent [[slides](https://canvas.umn.edu/files/50650734/download?download_frd=1)] ||  
|| 4.4 (2/19) || Proximal gradient descent [[slides](https://canvas.umn.edu/files/50744983/download?download_frd=1)] ||  
|| 4.5 (2/21) || Generalized gradient descent [[slides](https://canvas.umn.edu/files/50812047/download?download_frd=1)] || 
|| 4.6 (2/24) || Stochastic gradient descent [[slides](https://canvas.umn.edu/files/50888099/download?download_frd=1)] || 

Lecture 4 [[notes](https://canvas.umn.edu/files/51731831/download?download_frd=1)] 

------------------------

|| 5.1 (2/26) || Constrained optimization, KKT conditions [[slides](https://canvas.umn.edu/files/50983068/download?download_frd=1])] ||    
|| 5.2 (2/28) || Duality [[slides](https://canvas.umn.edu/files/51050924/download?download_frd=1)] ||    
|| 5.3 (3/3) || Barrier method [[slides](https://canvas.umn.edu/files/51139850/download?download_frd=1)] ||  
|| 5.4 (3/7) || Quadratic penalty method [[slides](https://canvas.umn.edu/files/51281771/download?download_frd=1)] || 
|| 5.5 (3/17) || ADMM [[slides](https://canvas.umn.edu/files/51427565/download?download_frd=1)] ||   
|| 5.6 (3/19) || ADMM continued [[slides](https://canvas.umn.edu/files/51510539/download?download_frd=1)] ||  
|| 5.7 (3/21) || Distance-to-set penalty method [[slides](https://canvas.umn.edu/files/51580292/download?download_frd=1)] ||    


Lecture 5 [[notes](https://canvas.umn.edu/files/52209025/download?download_frd=1)]

------------------------------

|| 6.1 (3/24) || Inversion method, rejection sampling [[slides](https://canvas.umn.edu/files/51667485/download?download_frd=1)] ||   
|| 6.2 (3/26) || Rejection sampling continued [[slides](https://canvas.umn.edu/files/51757939/download?download_frd=1)] ||   
|| 6.3 (3/28) || Importance sampling [[slides](https://canvas.umn.edu/files/51826902/download?download_frd=1)] || 


------------------------------


|| 7.1 (4/7) || Markov chains [[slides](https://canvas.umn.edu/files/52108392/download?download_frd=1)] ||    
|| 7.2 (4/9) || Markov chain convergence [[slides](https://canvas.umn.edu/files/52181987/download?download_frd=1)] ||
|| 7.3 (4/11) || Metropolis-Hastings algorithm [[slides](https://canvas.umn.edu/files/52244725/download?download_frd=1)] ||
|| 7.4 (4/14) || Gibbs sampling [[slides](https://canvas.umn.edu/files/52314349/download?download_frd=1)] ||   
|| 7.5 (4/16) || Gibbs sampling examples [[slides](https://canvas.umn.edu/files/52402945/download?download_frd=1)] ||  
|| 7.6 (4/18) || Practical MCMC [[slides](https://canvas.umn.edu/files/52506030/download?download_frd=1)]||  
|| 7.7 (4/21) || MALA and simulated annealing [[slides](https://canvas.umn.edu/files/52576843/download?download_frd=1)] || 
